购物比价
综合搜索
统计兴趣爱好
搜索引擎

resp = urlopen("https://en.wikipedia.org/wiki/Main_Page").read().decode("utf-8")
soup = bs(resp,"html.parser")
listUrls = soup.findAll("a", herf=re.compile("^/wiki/"))
for url in listUrls:
    print(url["herf"])

import urllib2
import re
from bs4 import BeautifulSoup
import pymysql

resp = urllib2.urlopen("http://baike.so.com/doc/1790119-1892991.html").read().decode("utf-8")
soup = BeautifulSoup(resp, "html.parser")
listUrls = soup.findAll("a", href = re.compile("^/doc/"))
for url in listUrls:
    print url.get_text(), "http://baike.so.com"+url["href"]
#获取数据库连接
connection = pymysql.connect(host='localhost',
                             user='root',
                             password='',
                             db='360mysql',
                             charset='utf8')
try:
#获取会话指针
    with connection.cursor() as cursor:
        for url in listUrls:
        #esc下
            sql = "insert into `urls`(`name`,`url`)values(%s,%s)"
            cursor.execute(sql,(url.get_text(),"http://baike.so.com"+url["href"]))
            connection.commit();
finally:
    connection.close();

utf8mb4


查询数据
1：得到记录总数
conut = cursor.execute()
2：查询下一行
cursor.fetchone()
3:得到指定大小
cursor.fetchmany(size =3)
4:得到所有
result =cursor.fetchall()

常见文档读取

1：urlopen()

乱码原因 

pdf读取

测试pdfminer3k安装成功
cd C:\Users\mtianyan\Desktop\pdfminer3k-1.3.1\pdfminer3k-1.3.1\tools
python pdf2txt.py ../samples/simple1.pdf

读取pdf文档

pdf文档————》分析器PDfParser

爬虫注意事项
robots协议

user -agent:表示指定爬虫*为通配符
disallow：不允许访问
allow:允许访问

针对搜索引擎：

1.明确禁止使用爬虫
2.分布器多线程爬虫
3.爬虫消耗对方的服务器

环境搭建

urllib

add_header()
多个add_header()
