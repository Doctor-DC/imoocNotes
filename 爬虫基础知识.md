##技术选型
>scrapy 可以加入requests和beautifulsoup
scrapy基于twisted，性能好
方便扩展,有很多内置功能
内置的`css`和`xpath selector`很方便，速度快

###网页分类
1. 静态网页
2. 动态页面
3. webservice(restapi)

###爬虫能做什么？
1. 搜索引擎：百度 垂直领域搜索引擎(只爬某领域)
2. 推荐引擎：今日头条
3. 机器学习的数据样本
4. 数据分析(金融数据分析) 舆情分析

###深度优先和广度优先
目录：
>1. 网站的树结构
2. 深度优先算法和实现
3. 广度优先算法和实现

网站url树结构:
分层设计
>子域名：
>1. bogbole.com
    1.1 blog.bogbole.com
    1.2 python.bogbole.com
        1.1.1 python.bogbole.com/123

环路链接：
>从首页到下面节点。
但是下面的链接节点又会有链接指向首页

所以：我们需要对于链接进行去重

1. 深度优先
2. 广度优先

>跳过已爬取的链接
对于二叉树的遍历问题

深度优先(递归实现)：
    顺着一条路，走到最深处。然后回头

广度优先(队列实现):
    分层遍历：遍历完儿子辈。然后遍历孙子辈

Python实现深度优先过程code：
```python
def depth_tree(tree_node):
    if tree_node is not None:
        print (tree_node._data)
        if tree_node._left is not None:
            return depth_tree(tree_node.left)
        if tree_node._right is not None:
            return depth_tree(tree_node,_right)
```
Python实现广度优先过程code：
```python
def level_queue(root):
    #利用队列实现树的广度优先遍历
    if root is None:
        return
    my_queue = []
    node = root
    my_queue.append(node)
    while my_queue:
        node = my_queue.pop(0)
        print (node.elem)
        if node.lchild is not None:
            my_queue.append(node.lchild)
        if node.rchild is not None:
            my_queue.append(node.rchild)
```

##爬虫去重策略
1. 将访问过的url保存到数据库中
2. 将url保存到set中。只需要O(1)的代价就可以查询到url
> 100000000*2byte*50个字符/1024/1024/1024 = 9G
3. url经过md5等方法哈希后保存到set中，将url压缩到固定长度而且不重复
4. 用bitmap方法，将访问过的url通过hash函数映射到某一位
5. bloomfilter方法对bitmap进行改进，多重hash函数降低冲突

scrapy去重使用的是第三种方法：后面分布式scrapy-redis会讲解bloomfilter方法。

###Python字符串编码问题解决：

1. 计算机只能处理数字，文本转换为数字才能处理，计算机中8个bit作为一个字节，
所以一个字节能表示的最大数字就是255
2. 计算机是美国人发明的，所以一个字节就可以标识所有单个字符
，所以ASCII(一个字节)编码就成为美国人的标准编码
3. 但是ASCII处理中文明显不够，中文不止255个汉字，所以中国制定了GB2312编码
，用两个字节表示一个汉字。GB2312将ASCII也包含进去了。同理，日文，韩文，越来越多的国家为了解决这个问题就都发展了一套编码，标准越来越多，如果出现多种语言混合显示就一定会出现乱码
4. 于是unicode出现了，它将所有语言包含进去了。
5. 看一下ASCII和unicode编码:
    1. 字母A用ASCII编码十进制是65，二进制 0100 0001
    2. 汉字"中" 已近超出ASCII编码的范围，用unicode编码是20013二进制是01001110 00101101
    3. A用unicode编码只需要前面补0二进制是 00000000 0100 0001
6. 乱码问题解决的，但是如果内容全是英文，unicode编码比ASCII编码需要多一倍的存储空间，传输也会变慢。
7. 所以此时出现了可变长的编码"utf-8" ,把英文：1字节，汉字3字节，特别生僻的变成4-6字节，如果传输大量的英文，utf8作用就很明显。

**读取文件，进行操作时转换为unicode编码进行处理**
**保存文件时，转换为utf-8编码。以便于传输**
读文件的库会将转换为unicode

*python2 默认编码格式为`ASCII`，Python3 默认编码为 `utf-8`*
```python
#python3
import sys
sys.getdefaultencoding()
s.encoding('utf-8')
```
```python
#python2
import sys
sys.getdefaultencoding()
s = "我和你"
su = u"我和你"
~~s.encode("utf-8")#会报错~~
s.decode("gb2312").encode("utf-8")
su.encode("utf-8")
```
###scrapy框架知识：
scrapy:
>Scrapy，Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。

伯乐在线：
子域名
blog.bole.com里的内容

all post的所有文章内容。如果网站直接提供抓取所有文章内容的url。

观察列表页。
url = http://blog.jobbole.com/all-posts/page/2/
>因此我们可以通过只更换url的参数来实现爬去所有

获取下一页url形式来爬去所有内容

基础环境python3.5.3 
创建所需scrapy虚拟环境:
```
pip install virtualenv
pip install virtualenvwrapper-win
安装虚拟环境管理
mkvirtualenv articlespider3
创建虚拟环境
workon testvir2
直接进入虚拟环境
```
>deactivate
退出激活状态
workon
知道有哪些虚拟环境

##虚拟环境内安装scrapy
>自行官网下载py35对应得whl文件进行pip离线安装

##命令行创建scrapy项目
```
scrapy startproject ArticleSpider
```

##scrapy目录结构
scrapy借鉴了django的项目思想
>`scrapy.cfg`：配置文件。
`setings.py`：设置

```
SPIDER_MODULES = ['ArticleSpider.spiders'] #存放spider的路径
NEWSPIDER_MODULE = 'ArticleSpider.spiders'
```

pipelines.py:
>做跟数据存储相关的东西

middilewares.py:
>自己定义的middlewares 定义方法，处理响应的IO操作

__init__.py:
>项目的初始化文件。

items.py：
>定义我们所要爬取的信息的相关属性。Item对象是种类似于表单，用来保存获取到的数据

###创建spider
```
cd ArticleSpider
scrapy genspider jobbole blog.jobbole.com
```

```
# -*- coding: utf-8 -*-
import scrapy


class JobboleSpider(scrapy.Spider):
    name = "jobbole"
    allowed_domains = ["blog.jobbole.com"]
    # start_urls是一个带爬的列表，
    #spider会为我们把请求下载网页做到，直接到parse阶段
    start_urls = ['http://blog.jobbole.com/']
    def parse(self, response):
```

scray 启动某一个Spyder的命令:
```
scrapy crawl jobbole
```
在windows报出错误：
`ImportError: No module named 'win32api'`

```
pip install pypiwin32#解决
```

在项目根目录里创建main.py
作为调试工具文件
```
# _*_ coding: utf-8 _*_
__author__ = 'mtianyan'
__date__ = '2017/3/28 12:06'

from scrapy.cmdline import execute

import sys
import os

#将系统当前目录设置为项目根目录
#os.path.abspath(__file__)为当前文件所在绝对路径
#os.path.dirname为文件所在目录
#H:\CodePath\spider\ArticleSpider\main.py
#H:\CodePath\spider\ArticleSpider
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
#执行命令，相当于在控制台cmd输入改名了
execute(["scrapy", "crawl" , "jobble"])
```
**settings.py的设置不遵守reboots协议**
`ROBOTSTXT_OBEY = False`

在jobble.py打上断点:
```
def parse(self, response):
    pass
```
可以看到他返回的htmlresponse对象:
对象内部：
- body:网页内容
- _DEFAULT_ENCODING= 'ascii'
- encoding= 'utf-8'

可以看出scrapy已经为我们做到了将网页下载下来。而且编码也进行了转换

##scrapy+xpath 提取伯乐在线网页内容-1

>提取目标伯乐在线

###xpath
目录:
1. xpath简介
2. xpath术语

>xpath使用路径表达式在xml和html中进行导航
xpath包含有一个标准函数库
xpath是一个w3c的标准

xpath节点关系

1. 父节点`*上一层节点*`
2. 子节点
3. 兄弟节点`*同胞节点*`
4. 先辈节点`*父节点，爷爷节点*`
5. 后代节点`*儿子，孙子*`
xpath语法:

| 表达式| 说明           | 
| ------------- |:-------------:| 
|article | 选取所有article元素的所有子节点 |
| /article|选取根元素article    |
| article/a| 选取所有属于article的子元素的a元素|
|//div|选取所有div元素（不管出现在文档里的任何地方）|
|article//div|选取所有属于article元素的后代的div元素，不管它出现在article之下的任何位置|
|//@class|选取所有名为class的属性|

xpath语法-谓语:

| 表达式| 说明           | 
| ------------- |:-------------:| 
|/article/div[1|选取属于article子元素的第一个div元素|
|/article/div[last()]|选取属于article子元素的最后一个div元素|
|/article/div[last()-1]|选取属于article子元素的倒数第二个div元素|
|//div[@color]|选取所有拥有color属性的div元素|
|//div[@color='red']|选取所有color属性值为red的div元素|

xpath语法:

| 表达式| 说明           | 
| ------------- |:-------------:| 
|/div/*|选取属于div元素的所有子节点|
|//*|选取所有元素|
|//div[@*]|选取所有带属性的div 元素|
|//div/a 丨//div/p|选取所有div元素的a和p元素|
|//span丨//ul|选取文档中的span和ul元素|
|article/div/p丨//span|选取所有属于article元素的div元素的p元素以及文档中所有的 span元素|

[firebugs插件](https://addons.mozilla.org/en-us/firefox/addon/firebug/?src=dp-dl-dependencies)

取某一个网页上元素的xpath地址
>如:http://blog.jobbole.com/110287/

在标题处右键使用firebugs查看元素。
然后在`<h1>2016 腾讯软件开发面试题（部分）</h1>`右键查看xpath

```python
# -*- coding: utf-8 -*-
import scrapy

class JobboleSpider(scrapy.Spider):
    name = "jobbole"
    allowed_domains = ["blog.jobbole.com"]
    start_urls = ['http://blog.jobbole.com/110287/']

    def parse(self, response):
        re_selector = response.xpath("/html/body/div[3]/div[3]/div[1]/div[1]/h1")
        # print(re_selector)
        pass
```
调试debug可以看到
re_selector =(selectorlist)[]
可以看到返回的是一个空列表，列表是为了如果我们当前的xpath路径下还有层级目录
可以进行选取
空说明没取到值：我们可以来chorme里观察一下
chorme取到的值
`//*[@id="post-110287"]/div[1]/h1`
chorme代码
```python
# -*- coding: utf-8 -*-
import scrapy


class JobboleSpider(scrapy.Spider):
    name = "jobbole"
    allowed_domains = ["blog.jobbole.com"]
    start_urls = ['http://blog.jobbole.com/110287/']

    def parse(self, response):
        re_selector = response.xpath('//*[@id="post-110287"]/div[1]/h1')
        # print(re_selector)
        pass
```
可以看出此时可以取到值
>分析页面，可以发现页面内有一部html是通过JavaScript ajax交互来生成的，因此在f12检查元素时的页面结构里有，而xpath不对
xpath是基于html源代码文件结构来找的

xpath可以有多种多样的写法：
```python
re_selector = response.xpath("/html/body/div[1]/div[3]/div[1]/div[1]/h1/text()")
re2_selector = response.xpath('//*[@id="post-110287"]/div[1]/h1/text()')
re3_selector = response.xpath('//div[@class="entry-header]/h1/text()')
```
推荐使用id型。因为页面id唯一。

通过了解了这些，下一篇文章将开始使用xpath利器抓取内容

cmd下运行python命令中文乱码解决方案
```
chcp 65001
```
###scrapy shell 运用
//*[@id="post-110704"]/div[2]/p
/html/body/div[1]/div/section[3]/div[1]/h3/b
```
scrapy shell http://blog.jobbole.com/110287/
>>>title = response.xpath('//*[@id="post-110287"]/div[1]/h1/text()')
```
//*[@id="post-110287"]/div[3]/div[9]/a/span/text()